{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c028884",
   "metadata": {},
   "source": [
    "# Tarea 2. Transformer con instrucciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3cdda",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1445ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch, torchaudio, glob\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from Trabajo_Utils import NoiseAug, RIRAug, identity, wer\n",
    "from Trabajo_Model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b844895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SEQ_LEN = 12\n",
    "NB_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def seed_everything(seed):      \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92cb94",
   "metadata": {},
   "source": [
    "### Data Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "828363f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"FluidInference/musan\", split=\"train\")\n",
    "\n",
    "# if not os.path.exists(\"musan_small\"):\n",
    "#     os.makedirs(\"musan_small\", exist_ok=True)\n",
    "#     for i, example in enumerate(dataset):\n",
    "#         audio = example[\"audio\"][\"array\"]\n",
    "#         sr = example[\"audio\"][\"sampling_rate\"]\n",
    "#         sf.write(f\"musan_small/file_{i}.wav\", audio, sr)\n",
    "        \n",
    "# url = 'https://openslr.elda.org/resources/28/rirs_noises.zip'\n",
    "\n",
    "# if not os.path.exists('RIRS_NOISES'):\n",
    "#     if not os.path.exists('rirs_noises.zip'):\n",
    "#         os.system('wget ' + url)\n",
    "#     os.system('unzip -q rirs_noises.zip')\n",
    "#     os.system('rm rirs_noises.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe2658",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "A continuación prepare un tokenizador que codifique mediante tokens especiales antes del mensaje de texto la acción que queremos ejecutar con el transformer de las 4 posibles: <br>\n",
    "- transcribe_es para transcribir directamente un audio en español a español <br>\n",
    "- transcribe_en para transcribir directamente un audio en inglés a inglés <br>\n",
    "- translate_en_es para traducir el mensaje con audio en inglés a español <br>\n",
    "- translate_es_en para traducir el mensaje con audio en español a inglés <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c3cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "    def __init__(self, csv_file):\n",
    "        # Token base\n",
    "        self.word2index = {\n",
    "            '<pad>': 0,\n",
    "            '<sos>': 1,\n",
    "            '<eos>': 2,\n",
    "            '<unk>': 3,\n",
    "\n",
    "            # Task tokens (Tarea 2)\n",
    "            '<transcribe_es>': 4,\n",
    "            '<transcribe_en>': 5,\n",
    "            '<translate_en_es>': 6,\n",
    "            '<translate_es_en>': 7,\n",
    "        }\n",
    "\n",
    "        self.index2word = {v: k for k, v in self.word2index.items()}\n",
    "\n",
    "        self.build_vocab(csv_file)\n",
    "\n",
    "    def build_vocab(self, csv_file):\n",
    "        with open(csv_file, encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                text = row['txt'].lower()\n",
    "                for word in text.split():\n",
    "                    if word not in self.word2index:\n",
    "                        idx = len(self.word2index)\n",
    "                        self.word2index[word] = idx\n",
    "                        self.index2word[idx] = word\n",
    "\n",
    "    def encode(self, action, text, seq_len):\n",
    "        \"\"\"\n",
    "        action: string, e.g. 'translate_es_en'\n",
    "        text: transcription/translation target\n",
    "        \"\"\"\n",
    "\n",
    "        task_token = f'<{action}>'\n",
    "\n",
    "        tokens = (\n",
    "            ['<sos>', task_token]\n",
    "            + text.lower().split()\n",
    "            + ['<eos>']\n",
    "        )\n",
    "\n",
    "        ids = [\n",
    "            self.word2index.get(t, self.word2index['<unk>'])\n",
    "            for t in tokens\n",
    "        ]\n",
    "\n",
    "        if len(ids) < seq_len:\n",
    "            ids += [self.word2index['<pad>']] * (seq_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:seq_len]\n",
    "\n",
    "        return torch.tensor(ids)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "\n",
    "        words = []\n",
    "        for i in ids:\n",
    "            w = self.index2word.get(i, '<unk>')\n",
    "            if w.startswith('<') and w.endswith('>'):\n",
    "                continue\n",
    "            words.append(w)\n",
    "\n",
    "        text = ' '.join(words)\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb68361",
   "metadata": {},
   "source": [
    "# 2.2\n",
    "Prepare un dataset/dataloader nuevo que utilizando los ficheros y transcripciones de la tarea anterior prepare una combinación de las cuatro acciones posibles con ejemplos suficientes para entrenar el transformer, es decir, todas las combinaciones de audio en español/inglés y texto español/inglés con la instrucción correspondiente antes del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f63b619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN instruct samples: 40000\n",
      "Saved to: fechas2/fechas2_train_instruct.csv\n"
     ]
    }
   ],
   "source": [
    "train_es = \"fechas2/fechas2_train.es.csv\"\n",
    "train_en = \"fechas2/fechas2_train.en.csv\"\n",
    "\n",
    "out_csv = \"fechas2/fechas2_train_instruct.csv\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "with open(train_es, encoding=\"utf-8\") as f_es, \\\n",
    "     open(train_en, encoding=\"utf-8\") as f_en:\n",
    "\n",
    "    reader_es = csv.DictReader(f_es)\n",
    "    reader_en = csv.DictReader(f_en)\n",
    "\n",
    "    for r_es, r_en in zip(reader_es, reader_en):\n",
    "\n",
    "        wav_es = r_es[\"wav\"]\n",
    "        wav_en = r_en[\"wav\"]\n",
    "\n",
    "        txt_es = r_es[\"txt\"]\n",
    "        txt_en = r_en[\"txt\"]\n",
    "\n",
    "        # Audio ES\n",
    "        rows.append([wav_es, \"transcribe_es\", txt_es])\n",
    "        rows.append([wav_es, \"translate_es_en\", txt_en])\n",
    "\n",
    "        # Audio EN\n",
    "        rows.append([wav_en, \"transcribe_en\", txt_en])\n",
    "        rows.append([wav_en, \"translate_en_es\", txt_es])\n",
    "\n",
    "with open(out_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"wav\", \"action\", \"txt\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(\"TRAIN instruct samples:\", len(rows))\n",
    "print(\"Saved to:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbee235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        tokenizer,\n",
    "        audio_len=4*16000,\n",
    "        seq_len=SEQ_LEN,\n",
    "        transform=[identity]\n",
    "    ):\n",
    "        self.audio_len = audio_len\n",
    "        self.seq_len = seq_len\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with open(csv_file, encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            self.data = [\n",
    "                (row['wav'], row['action'], row['txt'])\n",
    "                for row in reader\n",
    "            ]\n",
    "\n",
    "        print(\"Train samples:\", len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path, action, text = self.data[idx]\n",
    "\n",
    "        # --- Audio ---\n",
    "        x, fs = torchaudio.load(wav_path)\n",
    "\n",
    "        if x.shape[1] < self.audio_len:\n",
    "            x = torch.nn.functional.pad(\n",
    "                x, (0, self.audio_len - x.shape[1]), value=0\n",
    "            )\n",
    "        else:\n",
    "            x = x[:, :self.audio_len]\n",
    "\n",
    "        x = x[0].numpy()\n",
    "        for t in self.transform:\n",
    "            x = t(x)\n",
    "\n",
    "        # --- Target (ACTION + TEXT) ---\n",
    "        y = self.tokenizer.encode(\n",
    "            action=action,\n",
    "            text=text,\n",
    "            seq_len=self.seq_len\n",
    "        )\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        tokenizer,\n",
    "        audio_len=4*16000,\n",
    "        seq_len=SEQ_LEN\n",
    "    ):\n",
    "        self.audio_len = audio_len\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with open(csv_file, encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            self.data = [\n",
    "                (row['wav'], row['action'], row['txt'])\n",
    "                for row in reader\n",
    "            ]\n",
    "\n",
    "        print(\"Test samples:\", len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path, action, text = self.data[idx]\n",
    "\n",
    "        x, fs = torchaudio.load(wav_path)\n",
    "        if x.shape[1] < self.audio_len:\n",
    "            x = torch.nn.functional.pad(\n",
    "                x, (0, self.audio_len - x.shape[1]), value=0\n",
    "            )\n",
    "        else:\n",
    "            x = x[:, :self.audio_len]\n",
    "\n",
    "        x = x[0]\n",
    "\n",
    "        y = self.tokenizer.encode(\n",
    "            action=action,\n",
    "            text=text,\n",
    "            seq_len=self.seq_len\n",
    "        )\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9093a564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 40000\n",
      "Test samples: 1000\n"
     ]
    }
   ],
   "source": [
    "task_tokenizer = WordTokenizer(\n",
    "    csv_file='fechas2/fechas2_train_instruct.csv'\n",
    ")\n",
    "\n",
    "trainset = TrainDataset(\n",
    "    csv_file='fechas2/fechas2_train_instruct.csv',\n",
    "    tokenizer=task_tokenizer,\n",
    "    transform=[\n",
    "        NoiseAug(prob=0.5),\n",
    "        RIRAug(prob=0.5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "testset = TestDataset(\n",
    "    csv_file='fechas2/fechas2_test_instruct.csv',\n",
    "    tokenizer=task_tokenizer\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "vocab_size = len(task_tokenizer.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc1bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AudioTransformer(\n",
    "#     vocab_size=vocab_size,\n",
    "#     d_model=256,\n",
    "#     nb_layers=8,\n",
    "#     d_ff=512,\n",
    "#     n_heads=8,\n",
    "#     d_head=32,\n",
    "#     dropout=0.1,\n",
    "#     seq_len=SEQ_LEN\n",
    "# )\n",
    "\n",
    "# model.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "# pad_idx = task_tokenizer.word2index['<pad>']\n",
    "\n",
    "# import math\n",
    "\n",
    "# BATCH_FACTOR = 5\n",
    "# MAX_BATCH = None\n",
    "# if device == 'cpu':\n",
    "#     MAX_BATCH = math.ceil(len(trainloader) / BATCH_FACTOR)\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(NB_EPOCHS):\n",
    "#     total_loss = 0\n",
    "#     for i, (x, y) in enumerate(trainloader):\n",
    "#         if MAX_BATCH and i >= MAX_BATCH:\n",
    "#             break\n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = model.loss(x, y, pad_idx=pad_idx)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{NB_EPOCHS} - Loss: {total_loss/len(trainloader):.3f}\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"model_instruct.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716ebf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_instruction(model, x, action, tokenizer):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    sos = tokenizer.word2index['<sos>']\n",
    "    action_tok = tokenizer.word2index[f'<{action}>']\n",
    "    eos = tokenizer.word2index['<eos>']\n",
    "\n",
    "    y = [sos, action_tok]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc = model.encoder(x.unsqueeze(0).to(device))\n",
    "\n",
    "        while len(y) < SEQ_LEN and y[-1] != eos:\n",
    "            y_tensor = torch.tensor(y).unsqueeze(0).to(device)\n",
    "            logits = model.decoder(y_tensor, enc)\n",
    "            next_token = logits.argmax(-1)[0, -1].item()\n",
    "            y.append(next_token)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ad22e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioTransformer(\n",
       "  (fe): AudioFeatures(\n",
       "    (fe): MelSpectrogram(\n",
       "      (spectrogram): Spectrogram()\n",
       "      (mel_scale): MelScale()\n",
       "    )\n",
       "    (spec_aug): SpecAug()\n",
       "    (linear): Linear(in_features=80, out_features=256, bias=True)\n",
       "  )\n",
       "  (enc): Encoder(\n",
       "    (att): ModuleList(\n",
       "      (0-7): 8 x SelfAttention(\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (ff): ModuleList(\n",
       "      (0-7): 8 x FeedForward(\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "          (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (emb): Embedding(55, 256)\n",
       "  (dec): Decoder(\n",
       "    (att): ModuleList(\n",
       "      (0-7): 8 x CausalSelfAttention(\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (cross_att): ModuleList(\n",
       "      (0-7): 8 x CrossAttention(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (ff): ModuleList(\n",
       "      (0-7): 8 x FeedForward(\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "          (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=256, out_features=55, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AudioTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=256,\n",
    "    nb_layers=8,\n",
    "    d_ff=512,\n",
    "    n_heads=8,\n",
    "    d_head=32,\n",
    "    dropout=0.1,\n",
    "    seq_len=SEQ_LEN\n",
    ")\n",
    "\n",
    "state = torch.load(\"model_instruct.pt\", map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e76f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION: translate_es_en\n",
      "REF: the day after tomorrow thanks\n",
      "HYP: the day after tomorrow thanks\n",
      "----------------------------------------\n",
      "ACTION: transcribe_es\n",
      "REF: por favor en un par de días\n",
      "HYP: por favor en un par de días\n",
      "----------------------------------------\n",
      "ACTION: translate_en_es\n",
      "REF: por favor en un par de días\n",
      "HYP: por favor en un par de días\n",
      "----------------------------------------\n",
      "ACTION: translate_en_es\n",
      "REF: este jueves gracias\n",
      "HYP: este jueves gracias\n",
      "----------------------------------------\n",
      "ACTION: translate_en_es\n",
      "REF: por favor el miércoles siguiente\n",
      "HYP: por favor el siguiente miércoles\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x, y = testset[i]\n",
    "    wav, action, _ = testset.data[i]\n",
    "\n",
    "    ref = task_tokenizer.decode(y)\n",
    "    hyp = task_tokenizer.decode(\n",
    "        model.generate(x.unsqueeze(0), task_tokenizer, action)\n",
    "    )\n",
    "\n",
    "    print(f\"ACTION: {action}\")\n",
    "    print(\"REF:\", ref)\n",
    "    print(\"HYP:\", hyp)\n",
    "    print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c8a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_wer_instruct(model, dataset, tokenizer, gen_mode = 'greedy'):\n",
    "    total_wer = 0\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        x, y = dataset[i]\n",
    "        action = dataset.data[i][1]\n",
    "\n",
    "        ref = tokenizer.decode(y)\n",
    "\n",
    "        if gen_mode == 'greedy':\n",
    "            hyp_ids = model.generate(x.unsqueeze(0), tokenizer, action=action)\n",
    "        elif gen_mode == 'sampling':\n",
    "            hyp_ids = model.generate_sampling(x.unsqueeze(0), tokenizer, action=action)\n",
    "        else:\n",
    "            hyp_ids = model.generate_topk(x.unsqueeze(0), tokenizer, action=action)\n",
    "            \n",
    "        hyp = tokenizer.decode(hyp_ids)\n",
    "        total_wer += wer(ref, hyp)\n",
    "\n",
    "    return total_wer / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4461a1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER Tarea 2: 0.198\n",
      "WER Tarea 2 - sampling: 0.255\n",
      "WER Tarea 2 - topk: 0.267\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_instruct.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "wer_instr = evaluate_wer_instruct(model, testset, task_tokenizer, gen_mode='greedy')\n",
    "print(f\"WER Tarea 2: {wer_instr:.3f}\")\n",
    "\n",
    "wer_instr = evaluate_wer_instruct(model, testset, task_tokenizer, gen_mode='sampling')\n",
    "print(f\"WER Tarea 2 - sampling: {wer_instr:.3f}\")\n",
    "\n",
    "wer_instr = evaluate_wer_instruct(model, testset, task_tokenizer, gen_mode='topk')\n",
    "print(f\"WER Tarea 2 - topk: {wer_instr:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
