{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb814c3",
   "metadata": {},
   "source": [
    "# Terea 1: Reconocedor basado en whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fbfb98",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09621856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giuseppe/.pyenv/versions/hlt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch, torchaudio, glob\n",
    "import soundfile as sf\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e10a5b",
   "metadata": {},
   "source": [
    "#### Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7951d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 12\n",
    "\n",
    "def seed_everything(seed):      \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1081e",
   "metadata": {},
   "source": [
    "### Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07190f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"FluidInference/musan\", split=\"train\")\n",
    "\n",
    "if not os.path.exists(\"musan_small\"):\n",
    "    os.makedirs(\"musan_small\", exist_ok=True)\n",
    "    for i, example in enumerate(dataset):\n",
    "        audio = example[\"audio\"][\"array\"]\n",
    "        sr = example[\"audio\"][\"sampling_rate\"]\n",
    "        sf.write(f\"musan_small/file_{i}.wav\", audio, sr)\n",
    "        \n",
    "url = 'https://openslr.elda.org/resources/28/rirs_noises.zip'\n",
    "\n",
    "if not os.path.exists('RIRS_NOISES'):\n",
    "    if not os.path.exists('rirs_noises.zip'):\n",
    "        os.system('wget ' + url)\n",
    "    os.system('unzip -q rirs_noises.zip')\n",
    "    os.system('rm rirs_noises.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb074e",
   "metadata": {},
   "source": [
    "## 1.1\n",
    "Aprovechando lo visto en los cuadernos del taller se propone crear un tokenizador adaptado al vocabulario de la tarea. <br>\n",
    "La idea es que sea orientado a palabra ya que hay pocas y simplifica el reconocimiento. <br> \n",
    "Se puede reaprovechar el tokenizador\n",
    "DigitSumTokenizer que aparece en el cuaderno: parte2/run11. Implemente el\n",
    "tokenizador para la tarea: <br>\n",
    "- fechas2/fechas2_train.es.csv <br>\n",
    "- fechas2/fechas2_test.es.csv <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8b4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "    def __init__(self, csv_file):\n",
    "        self.word2index = {\n",
    "            '<pad>': 0,\n",
    "            '<sos>': 1,\n",
    "            '<eos>': 2\n",
    "        }\n",
    "        self.index2word = {\n",
    "            0: '<pad>',\n",
    "            1: '<sos>',\n",
    "            2: '<eos>'\n",
    "        }\n",
    "\n",
    "        self.build_vocab(csv_file)\n",
    "\n",
    "    def build_vocab(self, csv_file):\n",
    "        with open(csv_file, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                text = row['txt'].lower()\n",
    "                for word in text.split():\n",
    "                    if word not in self.word2index:\n",
    "                        idx = len(self.word2index)\n",
    "                        self.word2index[word] = idx\n",
    "                        self.index2word[idx] = word\n",
    "\n",
    "    def encode(self, text, seq_len=-1):\n",
    "        tokens = ['<sos>'] + text.lower().split() + ['<eos>']\n",
    "        ids = [self.word2index[w] for w in tokens]\n",
    "\n",
    "        if seq_len > len(ids):\n",
    "            ids += [self.word2index['<pad>']] * (seq_len - len(ids))\n",
    "\n",
    "        return torch.tensor(ids)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "\n",
    "        words = [self.index2word[i] for i in ids]\n",
    "        text = ' '.join(words)\n",
    "        text = text.replace('<sos>', '').replace('<eos>', '').replace('<pad>', '')\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca890e",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "Reutilice el contenido del cuaderno part1/run5 para implementar un Trainset y Testset adaptados a la tarea fechas2 que estamos implementando. <br> \n",
    "Compruebe de forma similar a como se hace en el cuaderno varios ficheros de train con aumento de ruido aditivo y reverberación con figuras de sus espectrogramas y la posibilidad de escucharlos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070b93f",
   "metadata": {},
   "source": [
    "#### Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63cd1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseAug(object):\n",
    "    def __init__(self, noise_dir='musan_small/', prob=0.5):\n",
    "        self.prob = prob\n",
    "        self.noises = glob.glob(noise_dir + '*.wav')\n",
    "\n",
    "        if len(self.noises) == 0:\n",
    "            print(\"[WARN] No noise files found, NoiseAug disabled.\")\n",
    "            self.prob = 0.0\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if len(self.noises) == 0:\n",
    "            return x\n",
    "\n",
    "        if np.random.uniform() < self.prob:\n",
    "            n = torchaudio.load(np.random.choice(self.noises))[0][0]\n",
    "\n",
    "            if len(n) < len(x):\n",
    "                n = torch.nn.functional.pad(n, (0, len(x)-len(n)))\n",
    "            elif len(n) > len(x):\n",
    "                t0 = np.random.randint(0, len(n) - len(x))\n",
    "                n = n[t0:t0+len(x)]\n",
    "\n",
    "            n = n.numpy()\n",
    "            snr = np.random.uniform(5, 15)\n",
    "            n = n * np.sqrt(x.std()**2 / (n.std()**2 + 1e-8)) * 10**(-snr/20)\n",
    "            x = x + n\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "class RIRAug(object):\n",
    "    def __init__(self, rir_dir='RIRS_NOISES/simulated_rirs', prob=0.5):\n",
    "        self.prob = prob\n",
    "        self.rirs = glob.glob(rir_dir + '/*.wav')\n",
    "\n",
    "        if len(self.rirs) == 0:\n",
    "            print(\"[WARN] No RIR files found, RIRAug disabled.\")\n",
    "            self.prob = 0.0\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if len(self.rirs) == 0:\n",
    "            return x\n",
    "\n",
    "        if np.random.uniform() < self.prob:\n",
    "            n = len(x)\n",
    "            rir = torchaudio.load(np.random.choice(self.rirs))[0][0]\n",
    "            rir = rir.numpy()\n",
    "            rir = rir / (np.max(np.abs(rir)) + 1e-8)\n",
    "            x = scipy.signal.convolve(x, rir)\n",
    "            t0 = np.argmax(np.abs(rir))\n",
    "            x = x[t0:t0+n]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01b4ac",
   "metadata": {},
   "source": [
    "#### Dataset Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02c519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        tokenizer,\n",
    "        audio_len=4*16000,\n",
    "        seq_len=SEQ_LEN,\n",
    "        transform=[identity]\n",
    "    ):\n",
    "        self.audio_len = audio_len\n",
    "        self.seq_len = seq_len\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with open(csv_file, encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            self.data = [(row['wav'], row['txt']) for row in reader]\n",
    "\n",
    "        print(len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path, text = self.data[idx]\n",
    "\n",
    "        x, fs = torchaudio.load(wav_path)\n",
    "\n",
    "        if x.shape[1] < self.audio_len:\n",
    "            x = torch.nn.functional.pad(\n",
    "                x, (0, self.audio_len - x.shape[1]), value=0\n",
    "            )\n",
    "        else:\n",
    "            x = x[:, :self.audio_len]\n",
    "\n",
    "        x = x[0].numpy()\n",
    "        for t in self.transform:\n",
    "            x = t(x)\n",
    "\n",
    "        y = self.tokenizer.encode(text, self.seq_len)\n",
    "        return x, y\n",
    "    \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, audio_len=4*16000, seq_len=SEQ_LEN):\n",
    "        self.audio_len = audio_len\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with open(csv_file, encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            self.data = [(row['wav'], row['txt']) for row in reader]\n",
    "\n",
    "        print(len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path, text = self.data[idx]\n",
    "\n",
    "        x, fs = torchaudio.load(wav_path)\n",
    "        if x.shape[1] < self.audio_len:\n",
    "            x = torch.nn.functional.pad(\n",
    "                x, (0, self.audio_len - x.shape[1]), value=0\n",
    "            )\n",
    "        else:\n",
    "            x = x[:, :self.audio_len]\n",
    "\n",
    "        x = x[0]\n",
    "        y = self.tokenizer.encode(text, self.seq_len)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b91ba762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] No RIR files found, RIRAug disabled.\n",
      "10000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordTokenizer('fechas2/fechas2_train.es.csv')\n",
    "\n",
    "trainset = TrainDataset(\n",
    "    'fechas2/fechas2_train.es.csv',\n",
    "    tokenizer,\n",
    "    transform=[\n",
    "        NoiseAug(prob=0.5),\n",
    "        RIRAug(prob=0.5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "testset = TestDataset(\n",
    "    'fechas2/fechas2_test.es.csv',\n",
    "    tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3037b",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "Reutilice el tokenizador de la tarea 1.1, el trainset/testset de la tarea 1.2 y el contenido del cuaderno part3/run21 para implementar el entrenamiento y test de la tarea de fechas con los ficheros: fechas2_train.es.csv y fechas2_test.es.csv. <br>\n",
    "Compruebe como se hace en el cuaderno la salida generada para algunos ejemplos de test y la visualización del modelo de atención y mida la tasa de error total WER en test. <br>\n",
    "Si el modelo se entrena muy lento por falta de GPU no hace falta que se hagan muchas iteraciones completas de los datos, lo que se valora es el código. <br>\n",
    "Puede usar también el frontend preentrenado que se muestra en el cuaderno parte3/run12 para mejorar las prestaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbeea372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=1024, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(d_model),\n",
    "            torch.nn.Linear(d_model, d_ff),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        b = x.shape[0]\n",
    "        q = self.q_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x).view(b, -1, self.n_heads, self.d_head) \n",
    "\n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale\n",
    "     \n",
    "        att = scores.softmax(dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, nb_layers=6, d_model=256, max_len=1000, **kwargs):\n",
    "        super().__init__()\n",
    "        self.pos = torch.nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        self.att = torch.nn.ModuleList([SelfAttention(d_model=d_model, **kwargs) for _ in range(nb_layers)])\n",
    "        self.ff = torch.nn.ModuleList([FeedForward(d_model=d_model, **kwargs) for _ in range(nb_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t, d = x.shape\n",
    "        x = x + self.pos[:, :t, :]\n",
    "        for att, ff in zip(self.att, self.ff):\n",
    "            x = x + att(x)\n",
    "            x = x + ff(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.seq_len = kwargs['seq_len']\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "        \n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.seq_len, self.seq_len))[None, None, ...] == 0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        b, n, d = x.shape\n",
    "        q = self.q_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x).view(b, -1, self.n_heads, self.d_head) \n",
    "\n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale\n",
    "        \n",
    "        scores = scores.masked_fill(self.mask[:,:,:n,:n], float('-inf'))\n",
    "        att = scores.softmax(dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "class CrossAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.norm1(x1)\n",
    "        x2 = self.norm2(x2)  \n",
    "        b = x1.shape[0]\n",
    "        q = self.q_linear(x1).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x2).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x2).view(b, -1, self.n_heads, self.d_head) \n",
    "\n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale\n",
    "     \n",
    "        att = scores.softmax(dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out, att\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, nb_layers=6, **kwargs):\n",
    "        super().__init__()        \n",
    "        self.seq_len = kwargs['seq_len']\n",
    "        self.pos = torch.nn.Parameter(torch.randn(1, self.seq_len, kwargs['d_model']))\n",
    "        self.att = torch.nn.ModuleList([CausalSelfAttention(**kwargs) for _ in range(nb_layers)])\n",
    "        self.cross_att = torch.nn.ModuleList([CrossAttention(**kwargs) for _ in range(nb_layers)])\n",
    "        self.ff = torch.nn.ModuleList([FeedForward(**kwargs) for _ in range(nb_layers)])\n",
    "        \n",
    "    def forward(self, x, enc):\n",
    "        b, t, d = x.shape\n",
    "        x = x + self.pos[:, :t, :]\n",
    "        for att, cross_att, ff in zip(self.att, self.cross_att, self.ff):\n",
    "            x = x + att(x)\n",
    "            x = x + cross_att(x, enc)[0]\n",
    "            x = x + ff(x)            \n",
    "        return x\n",
    "\n",
    "class SpecAug(torch.nn.Module):\n",
    "    def __init__(self, prob_t_warp=0.5,\n",
    "                       t_factor=(0.9, 1.1), \n",
    "                       f_mask_width = (0, 8), \n",
    "                       t_mask_width = (0, 10),\n",
    "                       nb_f_masks=[1,2], \n",
    "                       nb_t_masks=[1,2] ):\n",
    "        super().__init__()\n",
    "        self.t_factor = t_factor\n",
    "        self.f_mask_width = f_mask_width\n",
    "        self.t_mask_width = t_mask_width\n",
    "        self.nb_f_masks = nb_f_masks\n",
    "        self.nb_t_masks = nb_t_masks\n",
    "        self.prob_t_warp = prob_t_warp\n",
    "\n",
    "    def time_warp(self, x):\n",
    "        x = torch.nn.functional.interpolate(x, size=(int(x.shape[2]*np.random.uniform(*self.t_factor)), ))\n",
    "        # print('warp', x.shape[2])\n",
    "        return x\n",
    "    \n",
    "    def freq_mask(self, x):\n",
    "        for _ in range(np.random.randint(*self.nb_f_masks)):\n",
    "            f = np.random.randint(*self.f_mask_width)\n",
    "            f0 = np.random.randint(0, x.shape[1]-f)\n",
    "            # print('f', f0, f0+f)\n",
    "            x[:,f0:f0+f,:] = 0\n",
    "        return x\n",
    "\n",
    "    def time_mask(self, x):\n",
    "        for _ in range(np.random.randint(*self.nb_t_masks)):\n",
    "            t = np.random.randint(*self.t_mask_width)\n",
    "            t0 = np.random.randint(0, x.shape[2]-t)\n",
    "            # print('t', t0, t0+t)\n",
    "            x[:,:,t0:t0+t] = 0\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if np.random.uniform() < self.prob_t_warp:\n",
    "            x = self.time_warp(x)\n",
    "        x = self.freq_mask(x)\n",
    "        x = self.time_mask(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "399ba9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatures(torch.nn.Module):\n",
    "    def __init__(self, feat_dim=80, d_model=512, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fe = torchaudio.transforms.MelSpectrogram(\n",
    "                        n_fft=512, \n",
    "                        win_length=25*16, \n",
    "                        hop_length=10*16, \n",
    "                        n_mels=feat_dim)                            # 25ms window, 10ms shift\n",
    "        self.spec_aug = SpecAug()\n",
    "        self.linear = torch.nn.Linear(feat_dim, d_model)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.fe(x)        \n",
    "        x = (x+1e-6).log()\n",
    "        if self.training:\n",
    "            x = self.spec_aug(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class AudioTransformer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size=24, **kwargs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = kwargs['seq_len']\n",
    "\n",
    "        self.fe = AudioFeatures(**kwargs)\n",
    "        self.enc = Encoder(**kwargs)\n",
    "        \n",
    "        self.emb = torch.nn.Embedding(vocab_size, kwargs['d_model'])\n",
    "        self.dec = Decoder(**kwargs)\n",
    "        self.out = torch.nn.Linear(kwargs['d_model'], vocab_size)\n",
    "        \n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.fe(x)\n",
    "        return self.enc(x)\n",
    "\n",
    "    def decoder(self, y, enc):\n",
    "        y = self.emb(y)\n",
    "        dec = self.dec(y, enc)\n",
    "        return self.out(dec)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        enc = self.encoder(x)\n",
    "        return self.decoder(y, enc)\n",
    "                \n",
    "    def loss(self, x, y):        \n",
    "        logits = self(x, y[:,:-1])\n",
    "        target = y[:,1:]\n",
    "        loss = torch.nn.functional.cross_entropy(logits.reshape(-1, self.vocab_size), \n",
    "                                                 target.reshape(-1))\n",
    "        return loss\n",
    "    \n",
    "           \n",
    "    def generate(self, x):\n",
    "        device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "        y = [20,]\n",
    "        with torch.no_grad():            \n",
    "            enc = self.encoder(x.to(device))   \n",
    "            \n",
    "            while y[-1] != 22 and len(y) < self.seq_len:\n",
    "                logits = self.decoder(torch.tensor(y).unsqueeze(0).to(device), enc)\n",
    "                y.append(logits.argmax(-1)[:,-1].item())\n",
    "                            \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a8f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word2index)\n",
    "\n",
    "model = AudioTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=256,\n",
    "    nb_layers=8,\n",
    "    d_ff=512,\n",
    "    n_heads=8,\n",
    "    d_head=32,\n",
    "    dropout=0.1,\n",
    "    seq_len=SEQ_LEN\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "\n",
    "nb_epochs = 5\n",
    "batch_size = 32\n",
    "model.train()\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "for e in range(nb_epochs):\n",
    "    avg_loss = 0\n",
    "    for x, y in trainloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = model.loss(x, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        avg_loss += loss.item()\n",
    "    print('epoch %d/%d: avg_loss: %.2f' % (e,nb_epochs,avg_loss/len(trainloader)))\n",
    "       \n",
    "torch.save([model, opt], 'model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
